{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import warnings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = 'kaggle/input/apl-2025-spring-smoker-status'\n",
    "OUT_DIR = 'kaggle/output'\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = np.genfromtxt(f\"{IN_DIR}/train.csv\", delimiter=',', skip_header=1)\n",
    "testdata = np.genfromtxt(f\"{IN_DIR}/test.csv\", delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset = train_test_split(traindata, test_size=0.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = trainset[:, :-1], trainset[:, -1], valset[:, :-1], valset[:, -1]\n",
    "X_test = testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model/Pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Objective Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'auto',  # change to 'gpu_hist' if using GPU\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
    "        'verbosity': 0,\n",
    "    }\n",
    "\n",
    "    model = make_pipeline(StandardScaler(), xgb.XGBClassifier(**params))\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "    }\n",
    "\n",
    "    model = make_pipeline(StandardScaler(), RandomForestClassifier(**params))\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_objective(trial):\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 1e-4, 1e4, log=True),\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 1000),\n",
    "        'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "    }\n",
    "\n",
    "    model = make_pipeline(StandardScaler(), LogisticRegression(**params))\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ada_objective(trial):\n",
    "    base_estimator = DecisionTreeClassifier(max_depth=trial.suggest_int('base_max_depth', 2, 5))\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'base_estimator': base_estimator,\n",
    "    }\n",
    "\n",
    "    model = make_pipeline(StandardScaler(), AdaBoostClassifier(**params))\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('xgboost', xgb_objective, 10),\n",
    "    ('random_forest', rf_objective, 10),\n",
    "    ('logistic_regression', lr_objective, 10),\n",
    "    ('ada_boost', ada_objective, 10),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing xgboost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xgboost: 100%|██████████| 10/10 [00:12<00:00,  1.22s/it, score=0.8049, best=0.8122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial for xgboost:\n",
      "  Value: 0.8122\n",
      "  Params:\n",
      "    max_depth: 9\n",
      "    learning_rate: 0.01689528131437169\n",
      "    n_estimators: 288\n",
      "    lambda: 2.9706895135096407\n",
      "    alpha: 0.10045769879010309\n",
      "Optimizing random_forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "random_forest: 100%|██████████| 10/10 [00:18<00:00,  1.84s/it, score=0.7993, best=0.8018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial for random_forest:\n",
      "  Value: 0.8018\n",
      "  Params:\n",
      "    n_estimators: 492\n",
      "    max_depth: 9\n",
      "    min_samples_split: 7\n",
      "    min_samples_leaf: 6\n",
      "    max_features: log2\n",
      "Optimizing logistic_regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logistic_regression: 100%|██████████| 10/10 [00:01<00:00,  5.51it/s, score=0.7869, best=0.7869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial for logistic_regression:\n",
      "  Value: 0.7869\n",
      "  Params:\n",
      "    C: 1.3059668990720392\n",
      "    max_iter: 191\n",
      "    solver: liblinear\n",
      "    penalty: l2\n",
      "Optimizing ada_boost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ada_boost: 100%|██████████| 10/10 [00:49<00:00,  4.95s/it, score=0.8002, best=0.8064]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial for ada_boost:\n",
      "  Value: 0.8064\n",
      "  Params:\n",
      "    base_max_depth: 3\n",
      "    n_estimators: 156\n",
      "    learning_rate: 0.0502008338421205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'base_max_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-367-40caa43a4d7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ada_boost'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Unknown model name {name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\moalemm\\Documents\\AML\\applied-machine-learning\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'base_max_depth'"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "best_scores = []\n",
    "\n",
    "for name, objective, n_trials in models:\n",
    "    print(f\"Optimizing {name}...\")\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "\n",
    "    with tqdm(total=n_trials, desc=f\"{name}\") as pbar:\n",
    "        def make_callback(pbar):\n",
    "            def callback(study, trial):\n",
    "                score = trial.value\n",
    "                best_score = study.best_value\n",
    "                pbar.set_postfix({\"score\": f\"{score:.4f}\", \"best\": f\"{best_score:.4f}\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "            return callback\n",
    "\n",
    "        study.optimize(objective, n_trials=n_trials, callbacks=[make_callback(pbar)])\n",
    "\n",
    "    print(f\"\\nBest trial for {name}:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Value: {trial.value:.4f}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    if name == 'logistic_regression':\n",
    "        model = LogisticRegression(**trial.params)\n",
    "    elif name == 'random_forest':\n",
    "        model = RandomForestClassifier(**trial.params)\n",
    "    elif name == 'xgboost':\n",
    "        model = xgb.XGBClassifier(**trial.params, verbosity=0)\n",
    "    elif name == 'ada_boost':\n",
    "        # Extract AdaBoost params\n",
    "        ada_params = {\n",
    "            'n_estimators': trial.params['n_estimators'],\n",
    "            'learning_rate': trial.params['learning_rate'],\n",
    "        }\n",
    "\n",
    "        # Extract base estimator param\n",
    "        base_max_depth = trial.params['base_max_depth']\n",
    "        base_estimator = DecisionTreeClassifier(max_depth=base_max_depth)\n",
    "\n",
    "        model = AdaBoostClassifier(base_estimator=base_estimator, **ada_params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name {name}\")\n",
    "\n",
    "    best_models[name] = model\n",
    "    best_scores.append(trial.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all tuned models on the training data\n",
    "for model in best_models.values():\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('xgboost',\n",
       "                              XGBClassifier(alpha=9.844348377647071,\n",
       "                                            base_score=0.5, booster='gbtree',\n",
       "                                            colsample_bylevel=1,\n",
       "                                            colsample_bynode=1,\n",
       "                                            colsample_bytree=1,\n",
       "                                            enable_categorical=False, gamma=0,\n",
       "                                            gpu_id=-1, importance_type=None,\n",
       "                                            interaction_constraints='',\n",
       "                                            lambda=0.9872504954973608,\n",
       "                                            learning_rate=0.03481065336422548,\n",
       "                                            max_delta_step=0, max_depth=5,\n",
       "                                            mi...\n",
       "                                            validate_parameters=1,\n",
       "                                            verbosity=0)),\n",
       "                             ('random_forest',\n",
       "                              RandomForestClassifier(max_depth=10,\n",
       "                                                     max_features='sqrt',\n",
       "                                                     min_samples_leaf=3,\n",
       "                                                     min_samples_split=10,\n",
       "                                                     n_estimators=491)),\n",
       "                             ('logistic_regression',\n",
       "                              LogisticRegression(C=0.5348403941098272,\n",
       "                                                 max_iter=128, penalty='l1',\n",
       "                                                 solver='liblinear'))],\n",
       "                 voting='soft',\n",
       "                 weights=[0.8104444444444444, 0.8033333333333333,\n",
       "                          0.7871111111111111])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create weighted voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(name, model) for name, model in best_models.items()],\n",
    "    voting='soft',\n",
    "    weights=best_scores  # weights = list of Optuna best scores\n",
    ")\n",
    "\n",
    "# Fit voting ensemble\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted voting ensemble accuracy: 0.8102222222222222\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate\n",
    "val_pred = voting_clf.predict(X_val)\n",
    "print(\"Weighted voting ensemble accuracy:\", accuracy_score(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = voting_clf.predict(X_test).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = X_test[:, 0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = np.column_stack((test_ids, test_pred))\n",
    "\n",
    "np.savetxt(f\"{OUT_DIR}/smoking.csv\", sub_data, delimiter=',', header='id,smoking', comments='', fmt=['%d', '%d'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
